{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P05.CONV_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM1dD9EFOOLNICGQ7Kn0Wi0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcstllns/DeepLearning/blob/main/P05_CONV_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOU7uwWC_K-m"
      },
      "source": [
        "# Red convolucional con los datos de MNIST\n",
        "\n",
        "En esta red vamos a crear una red convolucional compuesta por dos capas convolucionales seguidas de dos capas de pooling.\n",
        "\n",
        "Los datos van a ser las imagenes en escala de grises de MNIST, los dígitos de 0 a 9 escritos a mano\n",
        "\n",
        "La tarea es la misma, clasificar correctamente cada imagen pero para hacerlo en vez de introducir una matriz one hot vamos a introducir las categorías directamente (números de 0 a 9). Ahora hay que cambiar la función de pérdida a __'sparse_categorical_crossentropy'__.\n",
        "\n",
        "El resto de los parámetros serán similares:\n",
        "\n",
        "1. Utilizaremos el optimizador adam con los tres parámetros\n",
        "1. Dividiremos el conjunto de entrenamiento en train y dev set\n",
        "1. Mediremos el tiempo de ejecución para ver cuánto tardamos en calcular la red\n",
        "\n",
        "Las redes convolucionales llevan mucho tiempo de cálculo, una estrategia bastante útil consiste en empezar con un subconjunto mas pequeño del conjunto de entrenamiento para comprobar que todo va bien. Luego cuando hayamos encontrado valores razonables de aprendizaje lo calcularemos con el conjunto completo.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a-klv2N_AfL"
      },
      "source": [
        "# Librerias necesarias\n",
        "\n",
        "import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "\n",
        "import numpy as np\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import pandas as pd\n",
        "# versiones de los paquetes\n",
        "print(tf.__version__)\n",
        "print(tf.keras.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "600U2zd1Gdkp"
      },
      "source": [
        "# Definimos la funcion para dibujar la historia\n",
        "\n",
        "def plot_history(history):\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('loss')\n",
        "  plt.plot(hist['epoch'], hist['loss'],'r--',\n",
        "           label='Training Error')\n",
        "  plt.plot(hist['epoch'], hist['val_loss'],'b',\n",
        "           label = 'Validation Error')\n",
        "  # plt.ylim([0,20])\n",
        "  plt.legend()\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BidnjNVBlDZ"
      },
      "source": [
        "Esta parte es similar a la de la práctica llamada MLP_mnist_onehot por lo que no explicaré mucho, solo los comentarios en el código"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsDH-geEA-S0"
      },
      "source": [
        "# carga de datos desde Keras, esto ya lo hemos visto en una práctica anterior\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# visualizamos la imagen 8\n",
        "plt.imshow(x_train[8])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUCmcnWLB3G3"
      },
      "source": [
        "# comprobamos las caracteristicas del set de entrenamiento\n",
        "print(x_train.ndim)\n",
        "print(x_train.shape)\n",
        "print(y_train.ndim)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFBRbADB6Aa"
      },
      "source": [
        "# guardamos las dimensiones en variables para hacer la red mas generica\n",
        "# y reutilizable\n",
        "\n",
        "ntrain = x_train.shape[0]\n",
        "ntest  = x_test.shape[0]\n",
        "dimf = x_train.shape[1]\n",
        "dimc = x_train.shape[2]\n",
        "\n",
        "print(\"dimensiones: \", ntrain, ntest, dimf, dimc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-MJyYULCH6j"
      },
      "source": [
        "## Preprocesado\n",
        "\n",
        "Ahora vamos a hacer el preprocesado, pero va a ser un poco diferente.\n",
        "\n",
        "1. Normalizamos las imágenes entre 0 y 1\n",
        "1. No vamos a vectorizarlas! las redes convolucionales necesitan que sean imágenes bidimensionales\n",
        "1. No vamos a hacer la codificación one hot, vamos a dejar y como está\n",
        "\n",
        "Keras espera que la entrada de cada caso en una red convolucional sea una matriz 3D en la que las dimensiones sean: filas x columnas x color\n",
        "\n",
        "Normalmente el color se especifica como un vector de tres elementos (RGB o YUV) o los elementos que requiera el espacio de color elegido, pero en nuestro caso solo son grises así que tendremos que la dimensión color solo tendrá un valor. Lo especificamos con el siguiente código:\n",
        "\n",
        "```\n",
        "x_train = x_train.reshape(ntrain, dimf, dimc, 1)\n",
        "x_test = x_test.reshape(ntest, dimf, dimc, 1)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX4GHfqdCDiM"
      },
      "source": [
        "# Preprocesamos los datos vectorizando y normalizando las imagenes\n",
        "\n",
        "x_train = x_train.astype('float32')/255.\n",
        "x_train = x_train.reshape(ntrain, dimf, dimc, 1)\n",
        "\n",
        "x_test = x_test.astype('float32')/255.\n",
        "x_test = x_test.reshape(ntest, dimf, dimc, 1)\n",
        "\n",
        "\n",
        "# comprobamos que la dimension de x es correcta\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8fA2XoMEEEv"
      },
      "source": [
        "Como el conjunto es muy numeroso vamos a crear un subconjunto aleatorio para hacer pruebas y luego calcularemos con el conjunto completo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emfYZAkzEMeJ"
      },
      "source": [
        "idx = np.random.randint(60000, size=1000) # elegimos 1000 imagenes de las 60000\n",
        "\n",
        "x_train_small = x_train[idx,:,:]\n",
        "y_train_small = y_train[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwD9alQNEiTr"
      },
      "source": [
        "Vamos a definir la red:\n",
        "\n",
        "```\n",
        "model.add(Conv2D(32, (5,5), activation='relu', input_shape=(28,28,1)))\n",
        "```\n",
        "Indica una capa convolucional con 32 kernels de dimensiones 5x5, la función de activación es 'relu' y la entrada es para cada caso una matriz de 28x28x1\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "```\n",
        "Se hace un pooling de la capa anterior usando una ventana de 2x2 y se elige el valor más alto de la ventana\n",
        "\n",
        "Por último se pone una capa densa de perceptrones con 'softmax' para hacer la clasificación. Para ello antes hemos tenido que 'aplanar' (pasar de matrices 2D a vectores 1D), si no, no se podrían aplicar los perceptrones.\n",
        "\n",
        "```\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXMEArI6EXy2"
      },
      "source": [
        "# MODEL --------------------------------------------------------------------\n",
        "# Definimos el modelo\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(Conv2D(32, (5,5), activation='relu', input_shape=(28,28,1)))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Conv2D(64, (5,5), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dwqOyQnFkbO"
      },
      "source": [
        "# Compilamos el modelo ------------------\n",
        "\n",
        "model.compile(\n",
        "    loss = 'sparse_categorical_crossentropy',\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.2, beta_1=0.1),\n",
        "    metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r6wLVIZFuIY"
      },
      "source": [
        "# Ajustamos el modelo -------------------\n",
        "# fijate que usamos el conjunto small para ser mas rapidos\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "history = model.fit(x_train_small, y_train_small,\n",
        "              batch_size=100,           # proporcional al conjunto de datos\n",
        "              epochs=30,\n",
        "              validation_split = 0.2,   # partimos en train y dev\n",
        "              verbose=0)                # para que no ocupe toda la pantalla\n",
        "end = time.time()\n",
        "print('Tiempo de ejecución:', end - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo0r2xyHGUmD"
      },
      "source": [
        "El tiempo de ejecución es alto. Hay que tenerlo en cuenta.\n",
        "\n",
        "Ahora evaluamos los resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JFIoP20J3Gf"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(x_train_small, y_train_small)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxFK25JQGmkl"
      },
      "source": [
        "plot_history(history)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6ey0GeUHj-9"
      },
      "source": [
        "Como ves el aprendizaje es un desastre. Prueba con distintos parámetros e hiperparámetros y cuando consideres que la red funciona correctamente puedes ajustarla con el set grande que contiene todas las imágenes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XirSO5SWtWl"
      },
      "source": [
        "# Ejercicio\n",
        "\n",
        "\n",
        "Una vez que hayas encontrado una buena red con una buena configuración de parámetros e hiperparámetros contesta el ejercicio de moodle\n",
        "\n"
      ]
    }
  ]
}