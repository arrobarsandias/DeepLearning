{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P04.MLP_regresion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOlTknDflUx/0lXaFpckvX+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcstllns/DeepLearning/blob/main/P04_MLP_regresion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNGQuprOkc18"
      },
      "source": [
        "# Perceptron Multicapa para una regresión lineal\n",
        "\n",
        "En este ejercicio vamos a crear una red para pronosticar el consumo de gasolina de los coches. Los datos provienen de la universidad de California y son ampliamente utilizados en los curos de Deep Learning.\n",
        "\n",
        "La información de los datos la tienes en este enlace, y son un conjunto de datos que contiene las siguientes variables:\n",
        "\n",
        "1. mpg: continuous\n",
        "2. cylinders: multi-valued discrete\n",
        "3. displacement: continuous\n",
        "4. horsepower: continuous\n",
        "5. weight: continuous\n",
        "6. acceleration: continuous\n",
        "7. model year: multi-valued discrete\n",
        "8. origin: multi-valued discrete\n",
        "9. car name: string (unique for each instance)\n",
        "\n",
        "El objetivo es pronosticar mpg que es el consumo de gasolina de los coches (millas por galón) basándose en las otras variables.\n",
        "\n",
        "La variable origin esta compuesta por tres valores:\n",
        "\n",
        "- 1: USA\n",
        "- 2: Europe\n",
        "- 3: Japan\n",
        "\n",
        "\n",
        "Así que lo mejor es hacer un one hot con ella.\n",
        "\n",
        "Como la variable MPG es continua vamos a utilizar un capa final lineal y como función de pérdida el mse.\n",
        "\n",
        "Además vamos a separar el conjunto de datos en tres: train, dev y test como se ha visto en la teoría y haremos unos gráficos para ver la evolución del aprendizaje y el ajuste final.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iIQA8AmlxNa"
      },
      "source": [
        "# paquetes y librerías que vamos a necesitar\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "import time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB05ZtT6smG5"
      },
      "source": [
        "# funciones que nos van a ayudar\n",
        "\n",
        "# normaliza un conjunto de datos\n",
        "def norm(x, st):\n",
        "    return((x - st['mean'])/st['std'])\n",
        "\n",
        "# un plot de la historia del ajuste\n",
        "def plot_history(history):\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Mean Square Error')\n",
        "  plt.plot(hist['epoch'], hist['mse'],'r--',\n",
        "           label='Training Error')\n",
        "  plt.plot(hist['epoch'], hist['val_mse'],'b',\n",
        "           label = 'Validation Error')\n",
        "  plt.ylim([0,20])\n",
        "  plt.legend()\n",
        "  plt.show() # un €\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YO9pfC2kRzK"
      },
      "source": [
        "# Cargamos los datos\n",
        "# Keras nos permite cargar directamente un fichero que este en internet \n",
        "\n",
        "# Especificamos la direccion donde esta el fichero y su nombre y como lo queremos guardar\n",
        "dataset_path = tf.keras.utils.get_file(\"auto-mpg.data\",\n",
        "                                       \"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")\n",
        "\n",
        "# El csv viene sin ninguna cabezera estos son los nombres de las variables\n",
        "column_names = ['MPG', 'Cylinders','Displacement','Horsepower','Weight','Acceleration','Model year','Origin']\n",
        "\n",
        "# se lee el fichero\n",
        "data = pd.read_csv(dataset_path, names=column_names, na_values=\"?\",\n",
        "                          comment='\\t', sep=\" \", skipinitialspace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WrtGd6kncam"
      },
      "source": [
        "# vemos las primeras filas de los datos\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMhaO1OnnlQZ"
      },
      "source": [
        "# PREPROCESAMIENTO --------------------\n",
        "\n",
        "# El fichero tiene datos perdidos y hay que eliminar las filas donde estan\n",
        "data = data.dropna() \n",
        "\n",
        "\n",
        "\n",
        "# La variable origin viene codificada de forma numerica pero la pasamos a un one hot\n",
        "\n",
        "# no te preocupes por este codigo, cosas de python\n",
        "o = data.pop('Origin')\n",
        "data['USA'] = (o == 1)*1.0\n",
        "data['Europe'] = (o == 2)*2.0\n",
        "data['Japan'] = (o == 3)*3.0\n",
        "\n",
        "# comprobamos que ahora tenemos una variable para cada pais de origen\n",
        "data.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uB99EjZ-okr0"
      },
      "source": [
        "# Como el train y test sets no estan creados los creamos ahora aleatoriamente\n",
        "\n",
        "x_train = data.sample(frac=0.8, random_state=0)\n",
        "x_test = data.drop(x_train.index)\n",
        "\n",
        "y_train = x_train.pop('MPG')\n",
        "y_test = x_test.pop('MPG')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOXrxSwbqAKx"
      },
      "source": [
        "x_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9omt6MSEqBRv"
      },
      "source": [
        "y_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9QyycczpDv9"
      },
      "source": [
        "Si quieres investiga un poco el código anterior para aprender un poco de python o si prefieres déjalo como está. Lo que hemos hecho es:\n",
        "1. x_train se crea eligiendo aleatoriamente el 80% de las dilas de data\n",
        "1. x_test son simplemente las que no han sido elegidas\n",
        "1. y_train e y_test son solo la variable MPG de los conjuntos de datos anteriores\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSbUrZEfrFUj"
      },
      "source": [
        "# como vemos las variables en x tienen valores muy dispares, vamos a normalizarlas\n",
        "# para que funcione adecuadamente el optimizador y no haya problemas de\n",
        "# exploding gradient\n",
        "\n",
        "# No es necesario normalizar y porque es la que intentamos predecir y queremos ver\n",
        "# cuanto nos estamos equivocando en la escala real"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NotKIrFwqbjn"
      },
      "source": [
        "# calculamos los estadisticos descriptivos para x_train\n",
        "x_train_stats = x_train.describe().transpose()\n",
        "x_train_stats.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjjBQk66q18v"
      },
      "source": [
        "# calculamos los estadisticos descriptivos para x_test\n",
        "x_test_stats = x_test.describe().transpose()\n",
        "x_test_stats.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf0Ulb-LsPmO"
      },
      "source": [
        "# normalizamos x_train y x_test\n",
        "\n",
        "x_train_norm = norm(x_train, x_train_stats)\n",
        "x_train_norm.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2BCEcprtFSv"
      },
      "source": [
        "x_test_norm = norm(x_test, x_test_stats)\n",
        "x_test_norm.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoqOZMLItgN3"
      },
      "source": [
        "# comprobamos que lo hemos hecho bien\n",
        "\n",
        "x_train_norm.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mylOdGfAtsAk"
      },
      "source": [
        "Está perfecto, las medias cercanas a cero (2.093159e-16) y las sd en 1.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCC-HBkBt0MA"
      },
      "source": [
        "# Modelo --------------------------\n",
        "\n",
        "\n",
        "# Configuramos la topología\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation = 'relu', input_shape=[( len(x_train_norm.keys()) )]))\n",
        "model.add(Dense(64, activation = 'relu'))\n",
        "model.add(Dense(64, activation = 'relu'))\n",
        "model.add(Dense(1, activation = 'linear'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0DGZ9_Euc2V"
      },
      "source": [
        "# compilamos\n",
        "\n",
        "model.compile(loss='mse',\n",
        "              optimizer = tf.keras.optimizers.RMSprop(learning_rate = 0.001),\n",
        "              metrics = ['mse','mae'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeBm4d1Zuk-r"
      },
      "source": [
        "# entrenamos\n",
        "start = time.time()\n",
        "history = model.fit(x_train_norm, y_train,\n",
        "                    epochs = 300,\n",
        "                    validation_split = 0.2,  # divide automaticamente el train\n",
        "                                             # en train (80%) y dev (20%)\n",
        "                    verbose = 0) # para evitar que se llene toda la pantalla\n",
        "\n",
        "end = time.time()\n",
        "print('Tiempo de ejecución:', end - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8yyCmNRvFLA"
      },
      "source": [
        "# Un grafico para ver el ajuste en los dos sets\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABOgAymyx9tx"
      },
      "source": [
        "# Evaluamos el modelo con el conjunto de validacion\n",
        "model.evaluate(x_test_norm, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOYfUGHnyYKF"
      },
      "source": [
        "# Como las variables son cuantitativas un grafico de dispersion podría estar perfecto\n",
        "\n",
        "yp = model.predict(x_test_norm)\n",
        "\n",
        "\n",
        "df = pd.DataFrame({'y': y_test, 'yp': yp.flatten()})\n",
        "df.plot.scatter(x='y', y='yp')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XirSO5SWtWl"
      },
      "source": [
        "# Ejercicio\n",
        "\n",
        "Prueba a modificar los hiperparámetros de la red e incluso su arquitectura para obtener una mayor velocidad en la red. Por ejemplo podrías hacer lo siguiente:\n",
        "\n",
        "1. Prueba a utilizar otros tipos de neuronas (RelU o tanh) y otros optimizadores para mejorar el tiempo de aprendizaje\n",
        "1. Simplifica al máximo la red pero sin una pérdida significativa de predicción\n",
        "\n",
        "--- \n",
        "\n",
        "Haz las pruebas que consideres interesantes e interpreta los resultados.\n",
        "\n",
        "Pega el código empleado (solo las partes que has cambiado) en la ventana de abajo\n",
        "\n",
        "Para ayudarte en las tareas simplemente usa google, te aportará miles de ejemplos\n",
        "\n",
        "Recuerda que los resultados tienes que subirlos a un cuestionario de moodle\n",
        "\n",
        "---\n",
        "\n",
        "__Opcional__: aprovechando que tenemos en df los valores de y_test y de yp (los pronosticados) calcula el R2 de esta recta que nos indicará la calidad del pronóstico usando un lenguaje más estadístico."
      ]
    }
  ]
}